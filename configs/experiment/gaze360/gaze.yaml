# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: gaze_combined
  - override /model: gazetr
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["gaze360_GFIE_MPS", "GazeTR"]

seed: 12345

trainer:
  # min_epochs: 10
  # max_epochs: 50
  # debug 
  # max_epochs: 5
  # overfit_batches: 3

callbacks:

  model_checkpoint:
    monitor: "val/angular_all"
    mode: "min"
  
  early_stopping: 
    # monitor: "val/angular_all"
    # patience: 20
    # mode: "min"

model:

  solver:
    lr: 0.0001
    weight_decay: 0.0001
    layer_decay: null
    warmup_epochs: 5
    scheduler: "cosine"
    apply_linear_scaling: True

  net:
    pretrained: true

data:
  batch_size: 120 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
  num_workers: 10
  data_to_cluster: False

logger:
  wandb:
    tags: ${tags}
    group: "gaze"
  aim:
    experiment: "gaze"

