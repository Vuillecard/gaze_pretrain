# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: gaze_combined
  - override /model: gazenet
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb
  - override /extras: default


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["gaze360", "GazeTR"]

seed: 12345

trainer:
  min_epochs: 10
  max_epochs: 50
  # debug 
  # max_epochs: 5
  # overfit_batches: 3
  # limit_test_batches: 10
  gradient_clip_val: 10

callbacks:

  model_checkpoint:
    monitor: "val/angular_Gaze360"
    mode: "min"
  
  early_stopping:
    monitor: "val/angular_all"
    patience: 20
    mode: "min"

extras: 
  bbox_scale_ratio: 0.1

model:
  solver:
    #lr: 0.001 spherical best 
    lr: 0.0003 #cartesian best
    weight_decay: 0.0001
    layer_decay: null
    warmup_epochs: 5
    scheduler: "cosine"
    apply_linear_scaling: True
  
  net:
    _target_: gaze_module.models.components.gaze_models.GazeNet
    encoder: 
      _target_: gaze_module.models.components.gaze_models.ResNet
      model_name: "resnet18"
      pretrained: True
    head: linear
    mode: cartesian

  loss:
    _target_: gaze_module.models.losses.GazeLoss
    main: 
      _target_: gaze_module.models.losses.AngularArcossLoss
    add:
      _target_: gaze_module.models.losses.PinBallLoss
      mode: cartesian
    beta: 1
  mode_angular: cartesian

data:
  datasets_train:
    - ${extras.gaze360}
  
  batch_size: 120 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
  num_workers: 10
  # batch_size: 120 # Needs to be divisible by the number of devices (e.g., if in a distributed setup)
  # num_workers: 10
  data_to_cluster: False

logger:
  wandb:
    tags: ${tags}
    group: "Gaze3_cart_pinball_arcoss_v2"
  aim:
    experiment: "gaze static resnet cartesian with different head crop size as inputs "

